{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install a few packages that aren't included in our Docker image\n",
    "\n",
    "!pip install --upgrade pip\n",
    "!apt-get -y install libxml2-dev libxslt-dev \n",
    "!pip3 install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /sharedfolder/nyt_articles_2017-04-12/\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('/sharedfolder/nyt_articles_2017-04-12/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "from newspaper import Article\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## Creating a function that uses the 'newspaper' package to download an article and return its text\n",
    "\n",
    "def url_to_article_text(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article_text = article.text.replace('\\n', ' ')\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a list of NYT section pages\n",
    "\n",
    "section_urls = [ \\\n",
    "'https://www.nytimes.com/section/health', \\\n",
    "'https://www.nytimes.com/section/arts', \\\n",
    "'https://www.nytimes.com/section/fashion', \\\n",
    "'https://www.nytimes.com/section/world', \\\n",
    "'https://www.nytimes.com/section/us', \\\n",
    "'https://www.nytimes.com/section/politics', \\\n",
    "'https://www.nytimes.com/section/nyregion', \\\n",
    "'https://www.nytimes.com/section/business', \\\n",
    "'https://www.nytimes.com/section/opinion', \\\n",
    "'https://www.nytimes.com/section/technology', \\\n",
    "'https://www.nytimes.com/section/science', \\\n",
    "'https://www.nytimes.com/section/health', \\\n",
    "'https://www.nytimes.com/section/sports', \\\n",
    "'https://www.nytimes.com/section/food', \\\n",
    "'https://www.nytimes.com/section/travel', \\\n",
    "'https://www.nytimes.com/section/t-magazine', \\\n",
    "'https://www.nytimes.com/section/magazine', \\\n",
    "'https://www.nytimes.com/section/realestate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffling the order of NYT pages\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(section_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Completing the scrape\n",
    "# We're downloading each section's articles to its own directory.\n",
    "\n",
    "for section_url in section_urls:\n",
    "    print(section_url)\n",
    "    \n",
    "    section_name = section_url.split('/')[-1]              ## Excerpting the current section name from the URL\n",
    "    \n",
    "    try: os.mkdir('/sharedfolder/nyt_articles_2017-04-12/' + section_name)   ## Creating the section directory, \n",
    "    except: pass                                                             # if it doesn't already exist\n",
    "    \n",
    "    os.chdir('/sharedfolder/nyt_articles_2017-04-12/' + section_name)        ## Changing to that directory\n",
    "    \n",
    "    html_text = urlopen(section_url)\n",
    "    soup = BeautifulSoup(html_text, 'lxml')                                            ## Parsing the page with BeautifulSoup\n",
    "    \n",
    "    urls = [item['href'] for item in soup.find_all('a') if '.html' in item['href']]    ## Extracting a list of link URLs\n",
    "\n",
    "    try: urls = random.sample(urls, 25)                                         ## Picking 25 article URLs at random\n",
    "    except: pass\n",
    "    \n",
    "    for url in urls:\n",
    "        try:\n",
    "            article_text = url_to_article_text(url)                 ## Using the article downloading function we created above  \n",
    "            filename = url.split('/')[-1].split('.html')[0]+'.txt'              ## Creating our new filename\n",
    "            if filename not in os.listdir('./'):                                ## Writing .txt file\n",
    "                with open(filename, 'w') as fo:\n",
    "                    fo.write(article_text)\n",
    "            time.sleep(0.5 + random.random())                                   ## Pausing before downloading next page\n",
    "        except Exception as e:\n",
    "            print('----')\n",
    "            print(url)\n",
    "            print(e)\n",
    "            print('----')\n",
    "    \n",
    "    time.sleep(3 + random.random())                                             ## Pausing before downloading next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grouping all articles in the same folder\n",
    "\n",
    "os.chdir('/sharedfolder/nyt_articles_2017-04-12/')\n",
    "\n",
    "!mkdir all_articles\n",
    "\n",
    "!mv ./*/*.txt ./all_articles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zipping up the whole collection\n",
    "\n",
    "os.chdir('/sharedfolder/')\n",
    "\n",
    "!zip -r nyt_articles_2017-04-12.zip nyt_articles_2017-04-12/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
