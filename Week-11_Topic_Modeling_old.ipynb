{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Unsupervised learning: Latent Dirichlet allocation (LDA) topic modeling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install a Python package for LDA\n",
    "# http://pythonhosted.org/lda/getting_started.html\n",
    "\n",
    "!pip3 install lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing basic packages\n",
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading 'Essays' by Ralph Waldo Emerson\n",
    "\n",
    "os.chdir('/sharedfolder/')\n",
    "\n",
    "!wget -N http://www.gutenberg.org/cache/epub/16643/pg16643.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the text\n",
    "\n",
    "text_path = 'pg16643.txt'\n",
    "\n",
    "text_data = open(text_path).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dividing the document into segments, with the aim of extracting individual essays\n",
    "\n",
    "len(text_data.split('\\n\\n\\n\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Viewing the beginning of each segment to determine which ones to keep\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for item in text_data.split('\\n\\n\\n\\n\\n'):\n",
    "    print('-----')\n",
    "    print(counter)\n",
    "    print(item[:80])\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a list of essays\n",
    "\n",
    "document_list = text_data.split('\\n\\n\\n\\n\\n')[9:20]\n",
    "\n",
    "print(len(document_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a vectorized representation of each essay in the list\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(document_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Viewing a single essay's vector\n",
    "\n",
    "sample_essay_vector = X.toarray()[3]\n",
    "\n",
    "print(len(sample_essay_vector))\n",
    "\n",
    "sample_essay_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a vocabulary list corresponding to the vectors we created above\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "vocabulary[8950:8980]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Viewing the 10 most frequent words in a single essay\n",
    "\n",
    "print(np.array(vocabulary)[np.argsort(sample_essay_vector)[::-1]][:10])\n",
    "\n",
    "print(np.argsort(sample_essay_vector)[::-1][:10]) # corresponding frequency values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing an LDA model: 10 topics and 1000 iterations\n",
    "\n",
    "import lda\n",
    "\n",
    "model = lda.LDA(n_topics=10, n_iter=1000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting the model using our list of vectors\n",
    "\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Viewing the top 50 words in each 'topic'\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "\n",
    "n_top_words = 50\n",
    "\n",
    "for i, topic_distribution in enumerate(topic_word):\n",
    "    topic_words = np.array(vocabulary)[np.argsort(topic_distribution)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating the process, removing stop words and punctuation first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize('We are symbols, and inhabit symbols.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing NLTK stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Python punctuation set\n",
    "\n",
    "import string\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing tokenization + stop word removal\n",
    "\n",
    "sentence = 'We are symbols, and inhabit symbols.'.lower()\n",
    "\n",
    "token_list = word_tokenize(sentence)\n",
    "\n",
    "sentence_filtered = [item for item in token_list if (item not in stop_words)&(item not in string.punctuation)]\n",
    "\n",
    "sentence_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing and removing stop words from our list of essays\n",
    "\n",
    "documents_filtered = []\n",
    "\n",
    "for document in document_list:\n",
    "    token_list = word_tokenize(document.lower())\n",
    "    tokens_filtered = [item for item in token_list if (item not in stop_words)&(item not in string.punctuation)]\n",
    "    documents_filtered.append(' '.join(tokens_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Viewing a segment of a preprocessed essay\n",
    "\n",
    "documents_filtered[3][2000:2100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorizing preprocessed essays\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(documents_filtered) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a vocabulary list corresponding to the vectors we created above\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "vocabulary[1140:1160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing an LDA model: 10 topics and 1000 iterations\n",
    "\n",
    "model = lda.LDA(n_topics=10, n_iter=1000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting the model using our list of vectors\n",
    "\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Viewing the top 50 words in each 'topic'\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "\n",
    "n_top_words = 50\n",
    "\n",
    "for i, topic_distribution in enumerate(topic_word):\n",
    "    topic_words = np.array(vocabulary)[np.argsort(topic_distribution)][:-(n_top_words+1):-1]\n",
    "    print('Topic ' + str(i) + ':')\n",
    "    print(' '.join(topic_words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â–·Assignment\n",
    "\n",
    "    Modify the code above: Apply a stemming step to each word before vectorizing the text.\n",
    "    See example stemming code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming example\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('nature'))\n",
    "\n",
    "print(stemmer.stem('natural'))\n",
    "\n",
    "print(stemmer.stem('naturalism'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
