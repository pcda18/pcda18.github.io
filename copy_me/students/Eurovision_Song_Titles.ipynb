{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update the 'pip' package manager and install the Beautiful Soup package\n",
    "\n",
    "!pip3 install --upgrade pip\n",
    "!pip3 install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the packages we'll use below\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "## URL for the Wikipedia page \"List of countries in the Eurovision Song Contest\"\n",
    "countries_url = \"https://en.wikipedia.org/wiki/List_of_countries_in_the_Eurovision_Song_Contest\"\n",
    "\n",
    "page_html = urlopen(countries_url).read().decode('utf8') ## Downloading the page's HTML source code \n",
    "                                                         # and storing it in the variable 'page_html'.\n",
    "    \n",
    "page_chunk = page_html.split('<dt>Table key</dt>')[1] ## Discarding the part of the page we don't need\n",
    "\n",
    "soup = BeautifulSoup(page_chunk, 'lxml') ## Parsing the HTML fragment with Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting HTML table to a list of lists\n",
    "\n",
    "country_lol = []\n",
    "\n",
    "for row in soup.find('table').find_all('tr'):\n",
    "    row_list = [item.get_text() for item in row.find_all('td')]\n",
    "    country_lol.append(row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a list of country names from list of lists\n",
    "\n",
    "country_names = [row[0].replace('\\xa0','') for row in country_lol if row!=[]]\n",
    "\n",
    "print(len(country_names))\n",
    "\n",
    "country_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a list of URLs for each country\n",
    "\n",
    "country_urls = []\n",
    "\n",
    "for row in soup.find('table').find_all('tr'):\n",
    "    item = row.find('td')\n",
    "    try:\n",
    "        url = item.find('a')['href'] ## Getting the URL from the link in each table cell\n",
    "        url = url.replace('/wiki/', 'https://en.wikipedia.org/wiki/') ## Converting relative links to absolute URLs\n",
    "        country_urls.append(url)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(country_urls))\n",
    "\n",
    "country_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing country names and URLs (to make sure they match)\n",
    "\n",
    "for i in range(len(country_names)):\n",
    "    print(country_names[i])\n",
    "    print(country_urls[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraping \"Contestants\" table from every country URL on our list\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "dataframe_list = []\n",
    "\n",
    "for i in range(len(country_urls)):\n",
    "    \n",
    "    country = country_names[i]\n",
    "    url = country_urls[i]\n",
    "    \n",
    "    page_html = urlopen(url).read().decode('utf8')\n",
    "\n",
    "    page_chunk = page_html.split('<span class=\"mw-headline\" id=\"Contestants\">')[1]\n",
    "\n",
    "    soup = BeautifulSoup(page_chunk, 'lxml')\n",
    "    \n",
    "    rows = soup.find('table').find_all('tr')\n",
    "\n",
    "    list_of_lists = []\n",
    "\n",
    "    header = [item.get_text() for item in soup.find('table').find_all('th')]\n",
    "    \n",
    "    header = [item.split('[')[0].strip() for item in header]\n",
    "    \n",
    "    if 'Language' not in header:\n",
    "        header.append('Language')\n",
    "    \n",
    "    for row in rows:\n",
    "        row = [item.get_text() for item in row.find_all('td')]\n",
    "        if len(row)>=4:\n",
    "            while len(row)<len(header):\n",
    "                row.append('')\n",
    "            list_of_lists.append(row)\n",
    "\n",
    "    dataframe = pd.DataFrame(list_of_lists, columns=header) ## We're using pandas because columns on Wikipedia\n",
    "                                                            # might be ordered differently.\n",
    "\n",
    "    dataframe['Country'] = country\n",
    "    \n",
    "    try:\n",
    "        reduced_dataframe = dataframe[['Country', 'Year', 'Language', 'Title']]\n",
    "        dataframe_list.append(reduced_dataframe)\n",
    "    except:\n",
    "            try:\n",
    "                reduced_dataframe = dataframe[['Country', 'Year', 'Language', 'Song']]\n",
    "                dataframe_list.append(reduced_dataframe)\n",
    "                reduced_dataframe.columns = ['Country', 'Year', 'Language', 'Title']\n",
    "            except Exception as e:\n",
    "                print('ERROR: ' + url)\n",
    "                print(header)\n",
    "                print(e)\n",
    "\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Viewing a random country's dataframe\n",
    "\n",
    "import random\n",
    "\n",
    "random.choice(dataframe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table = pd.concat(dataframe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/sharedfolder/')\n",
    "\n",
    "master_table.to_csv('Eurovision_Songs.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
